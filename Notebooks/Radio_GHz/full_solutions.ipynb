{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "from astropy.io import ascii\n",
    "import corner\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from scipy.stats import f\n",
    "#%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='radio_lightcurve.dat'):\n",
    "    data = ascii.read(filename)\n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot of the radio lightcurve\n",
    "Make a log-log plot of the flux density as a function of time. Make sure to modularise your code so that we can re-use parts of it later on. For bonus points use different markers for each telescope, and use a colour scale to denote the observation frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data(ax, sm, data, scaled=False, **kwargs):\n",
    "    telescope_marker_dict = {'VLA':'s', 'ATCA':'o', 'GMRT':'d'}\n",
    "    \n",
    "    \n",
    "    for row in data:\n",
    "        freq = row['frequency']\n",
    "        colorval = sm.to_rgba(freq)\n",
    "        \n",
    "        telescope = row['telescope']\n",
    "        marker = telescope_marker_dict[telescope]\n",
    "        \n",
    "        if scaled:\n",
    "            flux = row['scaled_flux']\n",
    "            rms = row['scaled_rms']\n",
    "        else:\n",
    "            flux = row['flux']\n",
    "            rms = row['rms']\n",
    "        \n",
    "        ax.errorbar(row['delta_t'], flux, rms, linestyle='', marker=marker, c=colorval, **kwargs)\n",
    "    return\n",
    "\n",
    "def cmap_setup(cmap='viridis', min_freq=0, max_freq=17):\n",
    "    freq_cmap = plt.cm.get_cmap(cmap)\n",
    "    \n",
    "    cNorm  = colors.Normalize(vmin=min_freq, vmax=max_freq)\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\n",
    "    sm = scalarMap\n",
    "    sm._A = []\n",
    "    \n",
    "    return sm    \n",
    "    \n",
    "def make_plot(data, scaled=False, model=None, params=None, tvals=np.arange(10,400), plot_models=False):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    sm = cmap_setup()\n",
    "    plot_data(ax, sm, data, scaled=scaled)\n",
    "    \n",
    "    cbar = fig.colorbar(sm,fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Frequency (GHz)')\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    ax.set_xlabel('Time (days)')\n",
    "    if scaled:\n",
    "        ax.set_ylabel('Scaled Flux Density ($\\mu$Jy)')\n",
    "    else:\n",
    "        ax.set_ylabel('Flux Density ($\\mu$Jy)')\n",
    "        \n",
    "    if model:\n",
    "        plot_model(model, params, tvals, ax)\n",
    "    \n",
    "    if plot_models:\n",
    "        plot_physical_models(ax)\n",
    "    \n",
    "    ax.set_xlim(10,300)\n",
    "    \n",
    "make_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the spectral index\n",
    "Write a function to take a subset of the data and calculate the spectral index and its uncertainty. Using multi-band observation at 162 days post-merger calculate the spectral index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_power_law(freq,S0,alpha):\n",
    "    S = S0 * (freq) ** alpha\n",
    "    return S\n",
    "\n",
    "def alpha_calc(data):\n",
    "    freqs = data['frequency']\n",
    "    flux = data['flux']\n",
    "    flux_errs = data['rms']\n",
    "    \n",
    "    popt, pcov = curve_fit(calc_power_law, freqs, flux ,sigma=flux_errs, p0=(50,-0.61),absolute_sigma=True)\n",
    "    \n",
    "    alpha = popt[1]\n",
    "    alpha_err = np.sqrt(np.diag(pcov))[1]\n",
    "    \n",
    "    return alpha, alpha_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_data = data[data['delta_t'] == 162.89]\n",
    "\n",
    "# alpha, alpha_err = alpha_calc(sel_data)\n",
    "\n",
    "# print(\"alpha = %.1f+/-%.1f\"%(alpha, alpha_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data based on the spectral Index\n",
    "Write a function to take the observed data and scale it to a specific frequency based on an estimated spectral index. Don't forget to include uncertainties! You should add two columns to your data table called \"scaled_flux\" and \"scaled_rms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_data(data, alpha, alpha_err, ref_freq=3.0):\n",
    "    f_scale = (ref_freq/data['frequency'])**alpha\n",
    "    rms_scale = np.abs(f_scale*np.log(ref_freq/data['frequency'])*alpha_err)\n",
    "    \n",
    "    scaled_flux = data['flux'] * f_scale\n",
    "    scaled_rms = np.abs(scaled_flux) * np.sqrt((data['rms']/data['flux'])**2 + (rms_scale/f_scale)**2)\n",
    "    \n",
    "    data['scaled_flux'] = scaled_flux\n",
    "    data['scaled_rms'] = scaled_rms\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify your plot_data() and make_plot() functions to add a keyword parameter \"scaled\" that is by default False. If scaled=True, plot_data() should plot the scaled data instead of the observed data.\n",
    "\n",
    "Scale the data to 3 GHz based on your estimated spectral index and associated uncertainty, then plot the scaled lightcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = scale_data(data, -0.6,0.1)\n",
    "make_plot(data, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the data\n",
    "We now want to characterise the radio lightcurve. You should be able to see that it initially rises according to a power law, peaks somewhere between 100 and 200 days post-merger and then declines according to a different power law.\n",
    "\n",
    "However, when we published the first paper demonstrating evidence of a turnover we only had observations up to 200 days post-merger. We will now determine evidence for a turnover using that subset of data.\n",
    "\n",
    "Create a new table called tdata with the data up to 200 days post-merger and plot the data using your make_plot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "data = scale_data(data, -0.6,0.05)\n",
    "tdata = data[data['delta_t'] < 200]\n",
    "\n",
    "make_plot(tdata, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this data with a \"smoothed broken power law\", which combines two power laws with a smoothing parameter around the break point. One functional form of this is given by\n",
    "\n",
    "$S(t) = F_{\\rm peak} \\left[ \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_1} + \\left(\\dfrac{t}{t_{\\rm peak}}\\right)^{-s\\delta_2}\\right]^{-1/s}$\n",
    "\n",
    "Write a function smooth_broken_power_law() that outputs a smoothed broken power law that also scales based on frequency and spectral index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_broken_power_law(t, nu, F_peak, t_peak, delta_1, delta_2, alpha, log_s, nu0=3.0):\n",
    "    s = 10**log_s\n",
    "\n",
    "    return (nu/nu0)**alpha * F_peak * ((t/t_peak)**(-s*delta_1) + (t/t_peak)**(-s*delta_2))**(-1.0/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining evidence for a turnover\n",
    "\n",
    "We now want to fit a smoothed broken power law to our data and see if there is evidence for a turnover in the radio lightcurve. In our paper we do this via a parameter grid-search to minimise $\\chi^2$. \n",
    "\n",
    "Here we will perform an MCMC fit using the emcee package, to determine lightcurve parameters and the spectral index of the source. First you will need to write 3 functions that define your Probability, Prior and Likelihood.\n",
    "\n",
    "We will use a uniform prior with $\\delta_1>0$ (since we require the lightcurve to initially rise), $0<t_{\\rm peak}<300$ (since our data only covers the period up to 200 days) and $s<100$. The parameters will be passed to the function via a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(theta):\n",
    "    F_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "\n",
    "    if 0.0 < t_peak < 300.0 and delta_1 > 0.0 and log_s < 3:\n",
    "        return 0.0\n",
    "    \n",
    "    else:\n",
    "        return -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write a likelihood function that takes the lightcurve parameters inside the tuple theta, along with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnlike(theta, t, nu, S, S_err):\n",
    "    F_peak, t_peak, delta_1, delta_2, alpha, log_s = theta\n",
    "\n",
    "    model = smooth_broken_power_law(t, nu, F_peak, t_peak, delta_1, delta_2, alpha, log_s)\n",
    "    inv_sigma2 = 1.0/S_err**2\n",
    "\n",
    "    return -0.5*(np.sum((S-model)**2*inv_sigma2 - np.log(inv_sigma2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function to calculate the marginal probability using the lnlike() and lnprior() functions you calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(theta, t, nu, S, S_err):\n",
    "    lp = lnprior(theta)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    return lp + lnlike(theta, t, nu, S, S_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit the data using the emcee package. The function get_starting_pos() provided below will set up an array of walker starting positions for given lightcurve parameters. Examine the lightcurve and estimate some reasonable values for these parameters and add them to the function.\n",
    "\n",
    "Now write a function called run_mcmc() that will load the observed data, take the starting position and then run the emcee Ensemple Sampler. Use a small number of iterations and walkers initially (100/20) to see how long the code takes to run on your laptop. Then increase both parameters to a larger number so that the algorithm takes ~90 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_starting_pos(nwalkers, ndim=6):\n",
    "    F_peak = 110\n",
    "    t_peak = 150\n",
    "    delta_1 = 0.8\n",
    "    delta_2 = -2\n",
    "    alpha = -0.6\n",
    "    log_s = 1\n",
    "    \n",
    "    pos = [np.asarray([F_peak, t_peak, delta_1, delta_2, alpha, log_s]) + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_mcmc(data, niters=1000, nthreads=1, nwalkers=200, ndim=6):\n",
    "    t = data['delta_t']\n",
    "    nu = data['frequency']\n",
    "    S = data['flux']\n",
    "    S_err = data['rms']\n",
    "    \n",
    "    pos = get_starting_pos(nwalkers, ndim=ndim)\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(t, nu, S, S_err),threads=nthreads)\n",
    "    \n",
    "    start = timer()\n",
    "    sampler.run_mcmc(pos, niters)\n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Computation time: %f s\"%(end-start))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to inspect our chain to see if our algorithm has converged to a reasonable solution. First, extract the chain from the sampler, and then write a function to make a figure showing how each walker moves around the parameter space. Your figure should have 6 subplots (1 for each dimension), iteration number on the x-axis and parameter value on the y-axis.\n",
    "\n",
    "MCMC algorithms typically use a burn-in phase, where the sampler is moving towards the optimum solution and not yet accurately sampling the parameter space. Add a parameter chain_cut to your function that plots a vertical line at the end of the burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in power\n",
      "  after removing the cwd from sys.path.\n",
      "/home/gaurav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in power\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "sampler = run_mcmc(tdata)\n",
    "chain = sampler.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_chain_plot(chain, chain_cut):\n",
    "    niters = chain.shape[1]\n",
    "    ndim = chain.shape[2]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim,1,sharex=True)\n",
    "    fig.set_size_inches(7, 20)\n",
    "    \n",
    "    param_names = ['$F_{{\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "\n",
    "    for i, (ax,param_name) in enumerate(zip(axes,param_names)):\n",
    "        ax.plot(chain[:,:,i].T,linestyle='-',color='k',alpha=0.3)\n",
    "        ax.set_ylabel(param_name)\n",
    "        ax.set_xlim(0,niters)\n",
    "        ax.axvline(chain_cut,c='r',linestyle='--')\n",
    "\n",
    "chain_cut = 200\n",
    "\n",
    "make_chain_plot(chain, chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that our algorithm is converging, and we know how long the burn-in takes we can begin to estimate parameters. The function below will make a corner plot from the good part of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_chain = chain[:, chain_cut:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Too few points to create valid contours\n",
      "WARNING:root:Too few points to create valid contours\n"
     ]
    }
   ],
   "source": [
    "def make_corner_plot(good_chain, savefile='corner.png'):\n",
    "    param_names = ['$F_{{\\\\rm peak}, 3\\.{\\\\rm GHz}}$', '$t_{{\\\\rm peak}}$','$\\\\delta_1$','$\\\\delta_2$', '$\\\\alpha$', '$\\\\log_{10}(s)$']\n",
    "    ndim = good_chain.shape[2]\n",
    "    \n",
    "    fig = corner.corner(good_chain.reshape((-1, ndim)), labels=param_names, quantiles=[0.16, 0.5, 0.84], show_titles=True)\n",
    "    plt.savefig(savefile)\n",
    "\n",
    "make_corner_plot(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will then extract the median and uncertainty (1 standard deviation) from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_params(chain):\n",
    "    ndim = chain.shape[2]\n",
    "    \n",
    "    chain = chain.reshape((-1, ndim))\n",
    "    vals = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(chain, [16, 50, 84],axis=0)))\n",
    "    \n",
    "    param_names = ['F_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "    \n",
    "    param_dict = dict(zip(param_names,vals))\n",
    "    \n",
    "    return param_dict\n",
    "    \n",
    "    \n",
    "best_params = get_best_params(good_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function, calc_chi2(), that will calculate the $\\chi^2$ for the fit. We will use this later to compare different lightcurve models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.7516999030056\n"
     ]
    }
   ],
   "source": [
    "def calc_chi2(best_params, param_names, model, data, nu0=3.0):\n",
    "    args = []\n",
    "    for param in param_names:\n",
    "        val = best_params[param][0]\n",
    "        args.append(val)\n",
    "\n",
    "    best_fit = model(data['delta_t'], nu0, *args)\n",
    "    \n",
    "    chi2 = np.sum((best_fit-data['scaled_flux'])**2/data['scaled_rms']**2)\n",
    "    \n",
    "    return chi2\n",
    "\n",
    "param_names = ['F_peak', 't_peak', 'delta_1', 'delta_2', 'alpha', 'log_s']\n",
    "\n",
    "chi2_best = calc_chi2(best_params, param_names, smooth_broken_power_law, tdata)\n",
    "print(chi2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot our best fit on top of the observational data.\n",
    "\n",
    "Write a function plot_model() that takes in a function that calculates the model fit (in this case, our smooth_broken_power_law function), the best parameters, an array of values to plot the model for and a matplotlib axis to plot it on. Modify your make_plot function to take in an optional argument for the model to plot. If the model is supplied your make_plot() function should call your plot_model() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model(model, params, tvals, ax):\n",
    "    best_fit = model(tvals, 3.0, *params)\n",
    "    \n",
    "    ax.plot(tvals,best_fit,marker='',linestyle='-',c='k',linewidth=1.5,zorder=0)\n",
    "\n",
    "    return\n",
    "\n",
    "args = []\n",
    "for param in param_names:\n",
    "    val = best_params[param][0]\n",
    "    args.append(val)\n",
    "\n",
    "plotting_data = scale_data(tdata, best_params['alpha'][0],np.max(best_params['alpha'][1:]))    \n",
    "    \n",
    "make_plot(plotting_data,scaled=True,model=smooth_broken_power_law,params=args, plot_models=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we know that the lightcurve has definitely turned over?\n",
    "\n",
    "We can perform a similar process as above to fit a standard power law to our data and then use an F-test to determine which model (turnover or no turnover) provides the best fit. We have provided a power_law() function that calculates the a power-law fit to the data. Now write a series of functions to perform an MCMC fit using a standard power law; lnprior_noturnover(), lnlike_noturnover(), lnprob_noturnover(), get_starting_pos_noturnover(), run_mcmc_noturnover()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_law(t, nu, F0, delta_1, alpha, nu0=3.0):\n",
    "    return (nu/nu0)**alpha * F0 * t**delta_1\n",
    "\n",
    "def lnprior_noturnover(theta):\n",
    "    F0, delta_1, alpha = theta\n",
    "\n",
    "    if delta_1 > 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    else:\n",
    "        return -np.inf\n",
    "    \n",
    "def lnlike_noturnover(theta, t, nu, S, S_err):\n",
    "    F0, delta_1, alpha = theta\n",
    "\n",
    "    model = power_law(t, nu, F0, delta_1, alpha)\n",
    "    inv_sigma2 = 1.0/S_err**2\n",
    "\n",
    "    return -0.5*(np.sum((S-model)**2*inv_sigma2 - np.log(inv_sigma2)))\n",
    "\n",
    "def lnprob_noturnover(theta, t, nu, S, S_err):\n",
    "    lp = lnprior_noturnover(theta)\n",
    "\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "\n",
    "    return lp + lnlike_noturnover(theta, t, nu, S, S_err)\n",
    "\n",
    "def get_starting_pos_noturnover(nwalkers, ndim=6):\n",
    "    F0 = 10\n",
    "    delta_1 = 0.8\n",
    "    alpha = -0.6\n",
    "    \n",
    "    pos = [np.asarray([F0, delta_1, alpha]) + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "    \n",
    "    return pos\n",
    "\n",
    "def run_mcmc_noturnover(data, niters=1000, nthreads=1, nwalkers=200, ndim=3):\n",
    "    t = data['delta_t']\n",
    "    nu = data['frequency']\n",
    "    S = data['flux']\n",
    "    S_err = data['rms']\n",
    "    \n",
    "    pos = get_starting_pos_noturnover(nwalkers, ndim=ndim)\n",
    "    \n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob_noturnover, args=(t, nu, S, S_err),threads=nthreads)\n",
    "    \n",
    "    start = timer()\n",
    "    sampler.run_mcmc(pos, niters)\n",
    "    end = timer()\n",
    "    \n",
    "    print(\"Computation time: %f s\"%(end-start))\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the sampler for the standard power law fit and write a function to plot the chains and calculate the length of the burn-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 123.513436 s\n"
     ]
    }
   ],
   "source": [
    "sampler_noturnover = run_mcmc_noturnover(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_chain_plot_noturnover(chain, chain_cut):\n",
    "    niters = chain.shape[1]\n",
    "    ndim = chain.shape[2]\n",
    "\n",
    "    fig, axes = plt.subplots(ndim,1,sharex=True)\n",
    "    fig.set_size_inches(7, 10)\n",
    "    \n",
    "    param_names = ['$F_{0}, 3\\.{\\\\rm GHz}}$','$\\\\delta_1$', '$\\\\alpha$']\n",
    "\n",
    "    for i, (ax,param_name) in enumerate(zip(axes,param_names)):\n",
    "        ax.plot(chain[:,:,i].T,linestyle='-',color='k',alpha=0.3)\n",
    "        ax.set_ylabel(param_name)\n",
    "        ax.set_xlim(0,niters)\n",
    "        ax.axvline(chain_cut,c='r',linestyle='--')\n",
    "\n",
    "chain_noturnover = sampler_noturnover.chain\n",
    "chain_cut_nt = 200\n",
    "\n",
    "make_chain_plot_noturnover(chain_noturnover, chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a corner plot for the standard power law fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_corner_plot_noturnover(good_chain, savefile='corner.png'):\n",
    "    param_names = ['$F_{0, 3\\.{\\\\rm GHz}}$', '$\\\\delta_1$', '$\\\\alpha$']\n",
    "    ndim = good_chain.shape[2]\n",
    "    \n",
    "    fig = corner.corner(good_chain.reshape((-1, ndim)), quantiles=[0.16, 0.5, 0.84], labels=param_names, show_titles=True)\n",
    "    plt.savefig(savefile)\n",
    "\n",
    "good_chain_nt = chain_noturnover[:, chain_cut_nt:, :]\n",
    "\n",
    "make_corner_plot_noturnover(good_chain_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the values of $\\delta_1$ and $\\alpha$ compare to the previous fit. How does the calculated radio spectral index compare to the joint X-ray/radio spectral index ($\\alpha=-0.585\\pm 0.005$)?\n",
    "\n",
    "Now write a function to get the best parameters for the standard power law fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_params_noturnover(chain):\n",
    "    ndim = chain.shape[2]\n",
    "    \n",
    "    chain = chain.reshape((-1, ndim))\n",
    "    vals = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*np.percentile(chain, [16, 50, 84],axis=0)))\n",
    "    \n",
    "    param_names = ['F0', 'delta_1', 'alpha']\n",
    "    \n",
    "    param_dict = dict(zip(param_names,vals))\n",
    "    \n",
    "    return param_dict\n",
    "    \n",
    "    \n",
    "best_params_nt = get_best_params_noturnover(good_chain_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the best parameters for the standard power law fit we can plot it over our data. Don't forget to scale the data based on the calculated best-fit spectral index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_names_nt = ['F0', 'delta_1', 'alpha']\n",
    "args_nt = []\n",
    "for param in param_names_nt:\n",
    "    val = best_params_nt[param][0]\n",
    "    args_nt.append(val)\n",
    "        \n",
    "\n",
    "\n",
    "plotting_data_nt = scale_data(tdata, best_params_nt['alpha'][0],np.max(best_params_nt['alpha'][1:]))    \n",
    "    \n",
    "make_plot(plotting_data_nt,scaled=True,model=power_law,params=args_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the $\\chi^2$ for the standard power law fit. We will then use this, and the previously calculated $\\chi^2$ to perform an F-test and determine which model we prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.12289877111037\n"
     ]
    }
   ],
   "source": [
    "chi2_nt = calc_chi2(best_params_nt, ['F0', 'delta_1', 'alpha'], power_law, tdata)\n",
    "print(chi2_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [F-test](https://en.wikipedia.org/wiki/F-test) is a generalised test that can be used to compare statistical models. In particular, it is useful when comparing two models where one is a restricted form of the other. Write a function calculate_ftest that calculates the F statistic for our two fits and then calculates the corresponding p-value. Hint: We have already imported the [scipy.stats F-distribution model](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f.html), and we can access the cumulative distribution function using f.cdf()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002541095903878743"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ftest(chi2_t, p_t, chi2_nt, p_nt, n):\n",
    "    F = ((chi2_nt-chi2_t)/chi2_t) * (n-p_t)/(p_t-p_nt)\n",
    "    \n",
    "    pval = f.cdf(F, p_nt, p_t)\n",
    "    \n",
    "    return 1-pval\n",
    "\n",
    "n = len(tdata)\n",
    "p_t = 6\n",
    "p_nt = 3\n",
    "\n",
    "calculate_ftest(chi2_best, p_t, chi2_nt, p_nt, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is preferred? With what confidence can we say that we prefer one model over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Lightcurve Parameters\n",
    "We're now going to use the full radio lightcurve to determine the best fitting parameters for the smooth broken power law. Load the full dataset and pass it to your previous run_mcmc() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in power\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 155.603547 s\n"
     ]
    }
   ],
   "source": [
    "data = load_data()\n",
    "\n",
    "full_sampler = run_mcmc(data)\n",
    "full_chain = full_sampler.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the chain and estimate the burn-in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_chain_cut = 200\n",
    "\n",
    "make_chain_plot(full_chain, full_chain_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_full_chain = full_chain[:, full_chain_cut:, :]\n",
    "make_corner_plot(good_full_chain, savefile='corner_fulldata.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the best parameters and the $\\chi^2$ for the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.16736277208541\n"
     ]
    }
   ],
   "source": [
    "full_params = get_best_params(good_full_chain)\n",
    "plotting_data = scale_data(data, full_params['alpha'][0],np.max(full_params['alpha'][1:]))\n",
    "\n",
    "chi2_full = calc_chi2(full_params, param_names, smooth_broken_power_law, plotting_data)\n",
    "\n",
    "print(chi2_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the full lightcurve. A physically motivated model of a relativistic jet and cocoon of ejecta can also be overplotted by using passing \"plot_models=True\" to the make_plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_args = []\n",
    "\n",
    "for param in param_names:\n",
    "    val = full_params[param][0]\n",
    "    full_args.append(val)\n",
    "    \n",
    "make_plot(plotting_data,scaled=True,model=smooth_broken_power_law,params=full_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function to plot the best fitting model for the 3 GHz data and call it from your make_plot() function when you pass plot_models=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_physical_models(ax, fname='jet_cocoon_contribution.txt'):\n",
    "    data = ascii.read(fname)\n",
    "    \n",
    "    ax.plot(data['t'], data['cocoon']+data['jet'], label='Narrow Jet + Cocoon', c='r', linestyle='--')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_plot(plotting_data, scaled=True, model=smooth_broken_power_law, params=full_args, plot_models=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
